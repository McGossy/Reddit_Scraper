{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/bruce/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import datetime\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "nltk.download('vader_lexicon')\n",
    "import string\n",
    "import pandas as pd\n",
    "import collections\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('reddit_info.json')\n",
    "info = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basically logs into the bot? I think\n",
    "def create_reddit_object():\n",
    "    reddit = praw.Reddit(client_id = info['client_id'],\n",
    "                        client_secret = info['client_secret'],\n",
    "                        user_agent = info['user_agent'],\n",
    "                        username = info['username'],\n",
    "                        password = info['password'])\n",
    "    return reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1000 posts may not be enough, so compile 1000 posts from each subreddit of choosing.\n",
    "def compile_subreddits(subreddits):\n",
    "    headlines = set()\n",
    "\n",
    "    reddit = create_reddit_object()\n",
    "\n",
    "    for i in range(len(subreddits)):\n",
    "        subreddit = reddit.subreddit(subreddits[i])\n",
    "        top_headlines_in_year = subreddit.top(\"year\", limit=None)\n",
    "\n",
    "        for submission in top_headlines_in_year:\n",
    "            headlines.add(submission.title)\n",
    "    \n",
    "    return headlines\n",
    "\n"
   ]
  },
  {
   "source": [
    "\n",
    "# Here I am compiling all posts from popular, news, all, politics, and worldnews subreddits.\n",
    "all_headlines = compile_subreddits([\"popular\", \"news\", \"all\", \"politics\", \"worldnews\"])\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' Remove punctuation and digits from a string. '''\n",
    "def _process_string(s):\n",
    "    no_digits = []\n",
    "\n",
    "    # Convert string to lowercase, remove punctuation.\n",
    "    raw_string = s.lower()\n",
    "    clean_string = raw_string.translate(str.maketrans('','', string.punctuation))\n",
    "\n",
    "    # Remove all digits from string.\n",
    "    for letter in clean_string:\n",
    "        if not letter.isdigit():\n",
    "            no_digits.append(letter)\n",
    "\n",
    "    # Create the final string.\n",
    "    result = ''.join(no_digits)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "988\n"
     ]
    }
   ],
   "source": [
    "'''Find the most common word'''\n",
    "def find_common_words():\n",
    "\n",
    "    reddit = create_reddit_object()\n",
    "\n",
    "    wordcounts = {}\n",
    "\n",
    "    # Loop through the set with submission titles, remove stop words, and get a dictionary of word counts\n",
    "    for submission in all_headlines:\n",
    "        for word in _process_string(submission).split():\n",
    "            if word not in stopwords.words('english'):\n",
    "                if len(word) > 2:\n",
    "                    if word not in wordcounts:\n",
    "                        wordcounts[word] = 1\n",
    "                    else:\n",
    "                        wordcounts[word] += 1\n",
    "    return wordcounts\n",
    "\n",
    "word_counter = collections.Counter(find_common_words())\n",
    "for word, count in word_counter.most_common(500):\n",
    "    print(word, \":\", count)\n",
    "\n",
    "find_common_words()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}